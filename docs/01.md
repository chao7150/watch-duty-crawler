各エピソードのサブタイトルを作品の公式サイトから自動で取得する仕組みを作るとしたらどうする？
管理者が登録するにはあまりにも作業が多すぎるので自動化したい

前提として、公式サイトのURLが与えられている作品は全体の1/3くらい
公式サイトのURL構造やHTML構造は作品ごとバラバラであるため、単一のスクリプトでは対応できない。AIの活用を検討する
サブタイトルの取得はベストエフォートでよい
アクティブユーザーのアクセス傾向に基づいて取得するサブタイトルに優先順位を付けて良い
サブタイトルは放送日の数日前に公開される

これらの要件を満たすシステム構成を考えてください
コピーする
システム概要
公式サイトから各エピソードのサブタイトルを自動取得するための、ベストエフォート型の収集パイプラインを構築する。
公式URLがある作品に対してのみ、アクセス状況に基づく優先度付きクローリングとAI解析を行い、成功したサブタイトルはDBに反映する。

コンポーネント構成
1. 作品メタデータ管理
作品ID、放送スケジュール、公式サイトURL（任意）を保持するDB。

放送予定日を基に、取得対象となるエピソード（放送2〜3日前）を絞り込む。

2. 取得ジョブスケジューラ
放送スケジュールとユーザーのアクセス傾向を入力に、次に取得すべき作品を優先度付きキューに投入。

作品のアクセス数や視聴予約件数などを重み付けし、需要の高い作品を頻繁にチェック。

放送数日前のみチェック対象とすることで負荷を低減。

3. クローラ (HTTP + Headless Browser)
優先度キューから作品を受け取り、公式サイトのHTML・動的コンテンツを取得。

構造が複雑なサイトやスクリプト生成型サイトに備え、Headlessブラウザ（例: Playwright）を利用可能にする。

1/3の作品のみURLが存在するため、URLがないものはスキップ。

4. AI解析サービス
取得したHTML/DOMやテキストをLLMに渡し、「指定エピソード番号のタイトルを抽出」するプロンプトを実行。

解析結果には信頼度スコアを付与し、低スコアや不明結果は再試行または保留。

5. 解析結果ストア
抽出したサブタイトルと取得日時、信頼度、元URLなどを保存。

既存データとの比較で更新があった場合のみ差分を反映。

未取得や低信頼度のものは管理者に通知したり、手動編集UIで補完できるようにする。

6. モニタリング／失敗時リトライ
同一エピソードで複数回失敗した場合や、放送直前まで未取得の場合はアラートを発行。

失敗ログや取得HTMLを保存し、モデル改善や手動分析に活用。

処理フロー（概要）
スケジューラが、放送予定日から逆算して取得タスクをキューに積む。
アクセス状況に応じて優先順位を設定。

クローラが優先度の高い作品から公式サイトを取得。
失敗した場合はバックオフ付きで再試行。

取得したHTML/DOMをAI解析サービスへ渡し、エピソード番号→サブタイトルを抽出。

結果を解析結果ストアに保存。信頼度低い場合は再解析のためキューへ戻す。

放送日前日までにサブタイトルが取得できなかったものは管理者に通知し、手動対応も可能にする。

優先度制御の例
priority
=
w
1
×
user_views
+
w
2
×
watchlist_count
−
w
3
×
days_until_air
priority=w 
1
​
 ×user_views+w 
2
​
 ×watchlist_count−w 
3
​
 ×days_until_air

放送日が近くアクセス数の多い作品ほど、優先度が高まる。

AI活用ポイント
パターン学習
サイトごとにHTML構造が異なるため、事前にサンプルページを学習させ、特徴的なDOM構造やキーワードを抽出する。

プロンプトエンジニアリング
「指定エピソード番号のタイトルを抽出し、タイトルだけ返す」よう指示。
複数エピソードが列挙されている場合でも対応可能。

フィードバックループ
解析精度が低いサイトは、実際の正解データを用いてモデルを再調整。

考慮事項
サイト構造の変化や取得失敗が頻発する場合は、手動ルールやXPath指定を併用する。

公式URLがない作品は自動化対象外だが、将来的にファンサイトやニュースサイトをデータソースに拡張する可能性を残す。

モバイル／デスクトップ向けURLが分かれている場合は、候補URLを列挙して試行する。

プライバシー・利用規約を遵守し、アクセス頻度を制限する。

この構成により、公式サイトの構造差異をAI解析で吸収しつつ、ユーザー需要に応じた効率的なサブタイトル取得が可能になります。

サイト構造の変化はほぼ起きないと想定して良いです
公式URLがない作品は自動化対象外にしたくありません。公式URLはほぼすべての作品に存在しますが、手動登録が必要なため、登録されていない場合があります。
AI解析でサブタイトルが正確に取得できるか疑問です。AIは自然言語で回答するため、回答を受け取るプログラムが回答の正確にどの部分がサブタイトルかわからない懸念があります。
AI解析は費用がかさむためなるべく回数を抑えたいです
コピーする
更新された前提
各作品の公式サイト構造はほぼ変化しない

公式URLはほぼ全作品に存在するが、未登録なだけ

AI解析は高コストかつ自然言語出力の信頼性に懸念あり

取得タスクの優先順位付けやベストエフォート方針は維持

改善方針
1. 公式URLの自動取得と管理
作品メタ情報に公式URLが無い場合は、以下を順に実行して候補URLを取得

公式ツイッター・配信プラットフォームのメタデータからURL抽出

検索エンジン or 公開APIで「作品名 + 公式サイト」を検索し候補を列挙

既知のポータル（例: 番組表、出版社サイト）に作品IDとの紐付けがあればそこから取得

候補URLを初回クロールし、HTML内に作品タイトルなどのキーワードが含まれるか判定して公式URLとして確定

確定URLは作品メタDBへ登録し以後利用

2. サブタイトル取得の基本戦略
サイト構造が安定している前提で、各公式サイトのDOMを解析し「スクレイパー定義」を作成

新規作品は以下の流れ

取得HTMLを人手や半自動ツールで解析し、CSSセレクタ/XPath/正規表現を登録

登録後はその定義を用いてサブタイトルを直接抽出

例外が発生した際のみ監視アラートを出す

3. AI解析の最小化
上記スクレイパーが失敗した場合のフォールバックとしてのみAIを使用

LLM呼び出し時はプロンプトに「JSONで返答し、titleキーにサブタイトルを格納する」と明記し、出力をパースしやすくする

取得HTMLとスクレイパー結果の差分を監視し、複数回失敗した作品のみAIへ送ることでコスト抑制

AI結果の信頼度低い場合は人手確認や再学習対象にする（ただし頻度を抑える）

4. 優先順位付け
作品のアクセス傾向＋放送日前のチェックを維持

公式URL未登録作品にも同じ優先度計算を行い、URL未確定の場合はURL探索タスクを優先して実行

5. 出力精度担保
スクレイパー定義による抽出結果は自動で正規化（全角/半角や特殊文字の統一）

AIフォールバックの出力は強制JSON形式 → title 以外の情報を無視

取得結果は過去データと照合し、不自然な差分（桁数異常や文字化け）を検出した場合は保留

処理フロー（改訂版）
公式URL探索フェーズ（必要時）

未登録作品についてURL候補検索 → HTML確認 → 公式URL決定＆DB登録

クローリング

放送予定日が近い作品から優先度キューへ

URL確定済みのサイトを取得し、登録済みスクレイパー定義でサブタイトルを抽出

フォールバックAI解析（失敗時のみ）

解析結果をJSON形式で取得しサブタイトル抽出

結果ストア

サブタイトル・取得日時・手段（スクレイパー/AI）・信頼度を保存

モニタリング

異常検出・未取得アラート → 人手対応やスクレイパー修正

まとめ
自動URL取得で対象作品を拡大し、人手登録を最小化

構造安定性を活かしたスクレイパー定義で多くのケースを機械的に処理

AIは最後の手段として使い、JSON指示で出力を機械可読にすることで信頼性とコストを両立

優先度付けとモニタリングでベストエフォートながらも効率的なサブタイトル取得が可能

「取得HTMLを人手や半自動ツールで解析し、CSSセレクタ/XPath/正規表現を登録」いやあ、これは無理です。とんでもない作業量になります。
「LLM呼び出し時はプロンプトに「JSONで返答し、titleキーにサブタイトルを格納する」と明記し、出力をパースしやすくする」これでもAIがJSONのみで返答する保証はありません。structured generationが利用できるAIサービスは存在しますか？（参考 https://engineers.ntt.com/entry/202503-structured-generation/entry）
コピーする
手作業によるCSS/XPath登録の回避
人手で各サイトの構造を調べてセレクタを登録するのは現実的ではないため、
スクレイピング→DOM解析→サブタイトル抽出をすべてAI側に委譲する構成が適しています。

汎用クローラで公式サイトのHTMLを取得（必要に応じてヘッドレスブラウザを使用）

HTML全体、あるいはエピソード一覧部分を抽出してLLMへ入力

LLMに「エピソード番号とサブタイトルをJSON形式で返す」よう指示し、得られた構造化データを保存

取得できなかった場合はバックオフ付きで再試行し、それでもダメなら管理者へ通知（ベストエフォート方針）

この方法ならサイトごとにCSS/XPathを設定する必要がなく、ドメイン知識を持たない運用担当でも対応が容易です。

Structured Generation が利用できるサービス
LLMにJSON Schemaや**関数定義（function calling）**を渡して、構造化出力を強制できるサービスが複数存在します。

OpenAI系モデル

gpt-4.1 などでは「Function calling」「JSON mode」を利用することで、指定したキーに必ず値を埋める形で返答させられます。

スキーマに一致しない応答は自動的に再生成されるため、JSONが壊れている確率を下げられます。

Anthropic Claude

「Tool Use」機能により、JSON Schemaや関数定義を渡して構造化返答を得られます。

Google Vertex AI (Gemini)

“Structured output” 機能でJSON Schemaを指定した形式の出力を生成可能です。

NTTの技術ブログで紹介されている方法

Guardrails等を使い、生成モデルがJSONスキーマを満たさない場合に自動修復や再生成を行う枠組みが紹介されています。

自前のLLMでも類似の制御を行えるため、ベンダーロックインを避けたい場合の選択肢になります。

運用イメージ
各作品について、放送日前にHTMLを取得しLLMへ送信

LLMが {"episode": n, "title": "…"}[] の形で確定サブタイトルを返す

形式不正・欠損時はサービス側の再生成機能やGuardrailsで補正

取得結果をDBに保存し、AI依存のため信頼度フィールドを付与しておく（後から人手で差分確認が可能）

構造化出力に対応したLLMを利用すれば、手作業を大幅に減らしつつ、サブタイトル取得の自動化をより安定して運用できます。

うんその方針で良さそう。一旦現時点の内容でissueを作成して
コピーする
Issue Title: 自動サブタイトル取得システムの設計・実装

Issue Body:

概要
公式サイトから各エピソードのサブタイトルを自動取得するベストエフォート型の仕組みを構築したい

公式URLが登録済みの作品は全体の約1/3で、URLやHTML構造は作品ごとに異なる

放送日の数日前にサブタイトルが公開されるため、利用者のアクセス傾向に基づき取得優先順位を付ける

要求事項
作品メタデータ（公式URL含む）の管理

放送日とアクセス傾向に基づく優先度付きクローリング

HTMLを取得し、AIによるサブタイトル抽出（成功しなかった作品は後続に回すベストエフォート）

解析結果・信頼度をDBに保存し、放送直前までに取得できなければアラート

将来的な手動補完やログ解析に備えて取得失敗の記録を残す

想定コンポーネント
作品メタデータDB（作品ID・放送日・公式URL など）

取得ジョブスケジューラ（優先度キュー）

クローラ（HTTP + Headless Browser対応）

AI解析サービス（HTML → エピソード番号 & サブタイトル抽出）

結果ストア & モニタリング（信頼度管理・失敗時リトライ/通知）

この方針で問題なければ、詳細な設計に進みます。